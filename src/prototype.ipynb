{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.metrics.distance import edit_distance,jaccard_distance\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import RGATConv,SAGEConv\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize,LabelEncoder\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from pickle import load as pkl_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_dataset = pd.read_csv(\"./datasets/final-ds.csv\").map(lambda s: s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "mov_dataset[\"original_language\"] = enc.fit_transform(mov_dataset[\"original_language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeds.pkl\",\"rb\") as embfile: embs = pkl_load(embfile)\n",
    "for k in embs: mov_dataset[k] = embs[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRecommender:\n",
    "    def __init__(self, dataset, gnn_params=None, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dataset = dataset\n",
    "        self.size = len(dataset[\"title\"])\n",
    "        self.weights = {\n",
    "            \"is_adult\": 1.0, \"original_language\": 2.0,\n",
    "            \"overview\": 3.0, \"tagline\": 2.0,\n",
    "            \"genres\": 4.0, \"year\": 1.5,\n",
    "            \"keywords\":3.5\n",
    "        }\n",
    "        self.gnn_params = {\n",
    "            \"hidden_dim\": 32,\n",
    "            \"num_classes\": 12,\n",
    "            \"num_relations\": 4,\n",
    "            \"learning_rate\": 0.006,\n",
    "            \"weight_decay\": 5e-4,\n",
    "            \"epochs\": 32\n",
    "        }\n",
    "        if gnn_params: self.gnn_params.update(gnn_params)\n",
    "        self._build_graph()\n",
    "        self._init_gnn()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embmodel = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(self.device)\n",
    "    def _get_feature_embeddings(self, feature, batch_size=128):\n",
    "        sentences = self.dataset[feature]\n",
    "        embs = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i + batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch, return_tensors=\"pt\", \n",
    "                padding=True, truncation=True, max_length=256\n",
    "            ).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.embmodel(**inputs)\n",
    "                batch_embs = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            embs.extend(batch_embs)\n",
    "        return np.array(embs)\n",
    "    def _build_graph(self, thres=0.35):\n",
    "        self.graph = nx.Graph()\n",
    "        for i in range(self.size): self.graph.add_node(i, title=self.dataset[\"title\"][i])\n",
    "        for i in range(self.size):\n",
    "            for j in range(i + 1, self.size):\n",
    "                scalar_sim = cosine_similarity(\n",
    "                    [self._get_scalar_features(i)],\n",
    "                    [self._get_scalar_features(j)]\n",
    "                )[0][0]\n",
    "                text_sim = sum(\n",
    "                    cosine_similarity(\n",
    "                        [self.dataset[feature][i].flatten() * self.weights[feature]],\n",
    "                        [self.dataset[feature][j].flatten() * self.weights[feature]]\n",
    "                    )[0][0]\n",
    "                    for feature in [\"overview\", \"tagline\", \"genres\"]\n",
    "                )\n",
    "                if (scalar_sim + text_sim) > thres: self.graph.add_edge(i, j, weight=scalar_sim + text_sim)\n",
    "        edge_index = torch.tensor(list(self.graph.edges)).t().contiguous().to(self.device)\n",
    "        edge_weight = torch.tensor(\n",
    "            [self.graph[u][v][\"weight\"] for u, v in self.graph.edges],\n",
    "            dtype=torch.float\n",
    "        ).to(self.device)\n",
    "        self.graph_data = Data(\n",
    "            x=self._get_node_features(),\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=torch.nn.functional.normalize(edge_weight, p=2, dim=0),\n",
    "            y=torch.randint(0, self.gnn_params[\"num_classes\"], (self.size,), dtype=torch.long).to(self.device),\n",
    "            train_mask=torch.ones(self.size, dtype=torch.bool).to(self.device)\n",
    "        )\n",
    "        self.graph_data.edge_type = torch.randint(\n",
    "            0, self.gnn_params[\"num_relations\"], \n",
    "            (edge_index.size(1),), dtype=torch.long\n",
    "        ).to(self.device)\n",
    "    def _get_scalar_features(self, idx):\n",
    "        return np.array([\n",
    "            self.dataset[\"is_adult\"][idx] * self.weights[\"is_adult\"],\n",
    "            self.dataset[\"original_language\"][idx] * self.weights[\"original_language\"],\n",
    "            # self.dataset[\"year\"][idx] * self.weights[\"year\"]\n",
    "        ])\n",
    "    def _get_node_features(self):\n",
    "        features = []\n",
    "        for i in range(self.size):\n",
    "            scalar = np.expand_dims(self._get_scalar_features(i), axis=0)\n",
    "            text = np.concatenate([\n",
    "                self.embs[feature][i] * self.weights[feature]\n",
    "                for feature in [\"overview\", \"tagline\", \"genres\"]\n",
    "            ])\n",
    "            combined = np.hstack([scalar, text.reshape(1, -1)])\n",
    "            features.append(combined)\n",
    "        return torch.nn.functional.normalize(\n",
    "            torch.tensor(np.vstack(features), dtype=torch.float).to(self.device),\n",
    "            p=2, dim=1\n",
    "        )\n",
    "    def _init_gnn(self):\n",
    "        self.model = SageRecNet(\n",
    "            input_dim=self.graph_data.x.size(1),\n",
    "            hidden_dim=self.gnn_params[\"hidden_dim\"],\n",
    "            output_dim=self.gnn_params[\"num_classes\"],\n",
    "            num_relations=self.gnn_params[\"num_relations\"]\n",
    "        ).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.gnn_params[\"learning_rate\"],\n",
    "            weight_decay=self.gnn_params[\"weight_decay\"]\n",
    "        )\n",
    "    def _node_clustering(self,clusters=8,use_gnn_embeddings=True):\n",
    "        if use_gnn_embeddings:\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(\n",
    "                    self.graph_data.x,\n",
    "                    self.graph_data.edge_index,\n",
    "                    self.graph_data.edge_type\n",
    "                ).cpu().numpy()\n",
    "        else: embeddings = self.graph_data.x.cpu().numpy()\n",
    "        kmeans = KMeans(n_clusters=clusters)\n",
    "        self.cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    def _compute_heuristics(self,kn=7):\n",
    "        # Use the maximum possible similarity as a base and then add a weighted factor based on the average similarity of neighbors\n",
    "        self.heuristics = {}\n",
    "        for node in self.graph.nodes():\n",
    "            neighbors = list(self.graph[node].items())\n",
    "            similarities = [data['weight'] for _, data in neighbors]\n",
    "            top_similarities = sorted(similarities, reverse=True)[:kn]\n",
    "            self.heuristics[node] = sum(top_similarities)/len(top_similarities) if neighbors else 0\n",
    "    def train(self):\n",
    "        loss_hist = []\n",
    "        for epoch in range(self.gnn_params[\"epochs\"]):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(\n",
    "                self.graph_data.x, \n",
    "                self.graph_data.edge_index,\n",
    "                self.graph_data.edge_type\n",
    "            )\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                out[self.graph_data.train_mask],\n",
    "                self.graph_data.y[self.graph_data.train_mask]\n",
    "            )\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(f'Epoch {epoch+1}/{self.gnn_params[\"epochs\"]}, Loss: {loss.item():.4f}')\n",
    "            if loss_hist and abs(loss_hist[-1] - loss.item()) < 0.01:\n",
    "                print(f\"Converged at epoch {epoch+1}\")\n",
    "                break\n",
    "            loss_hist.append(loss.item())\n",
    "    def gnn_recommendations(self, prompt_titles, k=4):\n",
    "        self.model.eval()\n",
    "        title_to_index = {self.dataset[\"title\"][idx].lower(): idx for idx in range(self.size)}\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(\n",
    "                self.graph_data.x,\n",
    "                self.graph_data.edge_index,\n",
    "                self.graph_data.edge_type\n",
    "            )\n",
    "        results = {}\n",
    "        for title in prompt_titles:\n",
    "            idx = title_to_index.get(title.lower())\n",
    "            if idx is None:\n",
    "                continue\n",
    "                \n",
    "            sims = torch.nn.functional.cosine_similarity(\n",
    "                embeddings[idx].unsqueeze(0),\n",
    "                embeddings\n",
    "            )\n",
    "            top_k = sims.argsort(descending=True)[1:k+1]\n",
    "            results[title] = [self.dataset[\"title\"][i] for i in top_k.cpu().numpy()]\n",
    "        return results\n",
    "    def a_star_recommendations(self, prompt_titles, k=4, max_depth=3,score_threshold=0.5, max_exploration=500):\n",
    "        title_to_index = {self.dataset[\"title\"][idx].lower(): idx for idx in range(self.size)}\n",
    "        start_indices = [title_to_index.get(title.lower()) for title in prompt_titles if title in title_to_index]\n",
    "        if not start_indices: return []\n",
    "        if not hasattr(self, 'heuristics'): self._compute_heuristics()\n",
    "        node_scores = defaultdict(dict)\n",
    "        exploration_count = 0\n",
    "        queue = []\n",
    "        for start_idx in start_indices:\n",
    "            heapq.heappush(queue, (-self.heuristics[start_idx],start_idx,0.0,0,start_idx))\n",
    "            node_scores[start_idx][start_idx] = 0.0\n",
    "        while queue and exploration_count < max_exploration:\n",
    "            current_f_neg, current_node, current_g, depth, origin_idx = heapq.heappop(queue)\n",
    "            current_f = -current_f_neg\n",
    "            exploration_count += 1\n",
    "            if current_f < score_threshold or depth > max_depth: continue\n",
    "            if current_node not in node_scores or \\\n",
    "            current_g > node_scores[current_node].get(origin_idx, -1): node_scores[current_node][origin_idx] = current_g\n",
    "            for neighbor in self.graph.neighbors(current_node):\n",
    "                edge_weight = self.graph[current_node][neighbor]['weight']\n",
    "                new_g = current_g + edge_weight\n",
    "                new_f = new_g + self.heuristics[neighbor]\n",
    "                if new_f >= score_threshold:\n",
    "                    if (neighbor not in node_scores) or \\\n",
    "                    (new_g > node_scores[neighbor].get(origin_idx, -1)):\n",
    "                        heapq.heappush(queue, (-new_f, neighbor, new_g, depth+1, origin_idx))\n",
    "        scored_nodes = []\n",
    "        for node, origins in node_scores.items():\n",
    "            if len(origins) < len(start_indices): continue\n",
    "            min_score = min(origins.values())\n",
    "            avg_score = sum(origins.values()) / len(origins)\n",
    "            combined_score = avg_score * min_score\n",
    "            scored_nodes.append((node, combined_score, min_score, avg_score))\n",
    "        scored_nodes.sort(key=lambda x: (-x[1], -x[2], -x[3]))\n",
    "        final_recs = [\n",
    "            n for n, *scores in scored_nodes \n",
    "            if n not in start_indices\n",
    "        ][:k]\n",
    "        return [self.dataset['title'][idx] for idx in final_recs]\n",
    "class SageRecNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_relations):\n",
    "        super(SageRecNet, self).__init__()\n",
    "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type=None):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        return self.conv2(x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = GraphRecommender(mov_dataset)\n",
    "recommender.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
