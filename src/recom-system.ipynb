{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mlenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv,RGATConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from pickle import load as pkl_load,dump as pkl_dump\n",
    "import numpy as np\n",
    "from random import sample as random_sample\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_dataset = pd.read_csv(\"../datasets/final-ds.csv\").map(lambda s: s.lower() if type(s) == str else s)\n",
    "ogl_enc = LabelEncoder()\n",
    "mov_dataset[\"original_language\"] = ogl_enc.fit_transform(mov_dataset[\"original_language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeds.pkl\",\"rb\") as embfile: embs = pkl_load(embfile)\n",
    "for k in embs: mov_dataset[k] = embs[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageRecNet(torch.nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,num_relations):\n",
    "        super(SageRecNet,self).__init__()\n",
    "        self.conv1 = SAGEConv(input_dim,hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim,output_dim)\n",
    "    def forward(self,x,edge_index,edge_type=None):\n",
    "        x = self.conv1(x,edge_index)\n",
    "        x = torch.relu(x)\n",
    "        return self.conv2(x,edge_index)\n",
    "class RLRecommendationAgent(torch.nn.Module):\n",
    "    def __init__(self,user_dim,input_dim,num_actions,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.policy_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + user_dim,128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,1)\n",
    "        ).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(),lr=0.004)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self,user_context,item_embeddings):\n",
    "        expanded_context = user_context.unsqueeze(0).expand(item_embeddings.size(0),-1)\n",
    "        combined = torch.cat([expanded_context,item_embeddings],dim=1)\n",
    "        return self.policy_net(combined)\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"policy_net\": self.policy_net.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict()\n",
    "        }\n",
    "    def load_state_dict(self,state_dict):\n",
    "        self.policy_net.load_state_dict(state_dict[\"policy_net\"])\n",
    "        self.optimizer.load_state_dict(state_dict[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRecommender:\n",
    "    def __init__(self,dataset,gnn_params=None,use_clusters=True,device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dataset = dataset\n",
    "        self.size = len(dataset[\"title\"])\n",
    "        cn = 12\n",
    "        self.weights = {\n",
    "            \"is_adult\": 1.0,\"original_language\": 2.0,\n",
    "            \"overview\": 3.0,\"tagline\": 2.0,\n",
    "            \"genres\": 4.0,\"year\": 1.5,\n",
    "            \"keywords\":3.5\n",
    "        }\n",
    "        self.gnn_params = {\n",
    "            \"hidden_dim\": 128,\n",
    "            \"num_classes\": cn,\n",
    "            \"num_relations\": 4,\n",
    "            \"learning_rate\": 0.006,\n",
    "            \"weight_decay\": 5e-4,\n",
    "            \"epochs\": 64\n",
    "        }\n",
    "        self.title_to_index = {self.dataset[\"title\"][idx].lower(): idx for idx in range(self.size)}\n",
    "        self.le = LabelEncoder()\n",
    "        scalar_features = np.array([self._get_scalar_features(i) for i in range(self.size)])\n",
    "        text_features = []\n",
    "        for feature in [\"overview\",\"tagline\",\"genres\"]:\n",
    "            feat_array = np.stack(self.dataset[feature])\n",
    "            if feat_array.ndim == 3: feat_array = feat_array.squeeze(axis=1)\n",
    "            text_features.append(feat_array)\n",
    "        text_features = np.hstack(text_features)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=cn)\n",
    "        self.dataset.loc[:,\"labels\"] = kmeans.fit_predict(\n",
    "            np.hstack([scalar_features,text_features])\n",
    "        )\n",
    "        self.gnn_params[\"num_classes\"] = cn\n",
    "        self._build_graph()\n",
    "        self._init_gnn()\n",
    "        if gnn_params: self.gnn_params.update(gnn_params)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embmodel = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(self.device)\n",
    "        with torch.no_grad(): samp_out = self.model(self.graph_data.x,self.graph_data.edge_index)\n",
    "        self.rl_agent = RLRecommendationAgent(\n",
    "            user_dim=self.gnn_params[\"hidden_dim\"],\n",
    "            input_dim=samp_out.size(1),\n",
    "            num_actions=cn,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.user_feedback = defaultdict(list)\n",
    "        self.reward_history = []\n",
    "        self.epsilon = 0.2\n",
    "    def _get_feature_embeddings(self,feature,batch_size=128):\n",
    "        sentences = self.dataset[feature]\n",
    "        embs = []\n",
    "        for i in range(0,len(sentences),batch_size):\n",
    "            batch = sentences[i:i + batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch,return_tensors=\"pt\",\n",
    "                padding=True,truncation=True,max_length=256\n",
    "            ).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.embmodel(**inputs)\n",
    "                batch_embs = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            embs.extend(batch_embs)\n",
    "        return np.array(embs)\n",
    "    def _build_graph(self,thres=0.35,neighbors=70):\n",
    "        self.graph = nx.Graph()\n",
    "        scalar_features = np.array([self._get_scalar_features(i) for i in range(self.size)])\n",
    "        text_features = []\n",
    "        for feature in [\"overview\",\"tagline\",\"genres\"]:\n",
    "            feat_array = np.stack(self.dataset[feature])\n",
    "            if feat_array.ndim == 3: feat_array = feat_array.squeeze(axis=1)\n",
    "            text_features.append(feat_array)\n",
    "        text_features = np.hstack(text_features)\n",
    "        all_features = np.hstack([scalar_features,text_features])\n",
    "        all_features = all_features / np.linalg.norm(all_features,axis=1,keepdims=True)\n",
    "        nbrs = NearestNeighbors(n_neighbors=neighbors,metric=\"cosine\",n_jobs=4).fit(all_features)\n",
    "        distances,indices = nbrs.kneighbors(all_features)\n",
    "        similarities = 1 - distances\n",
    "        edges = []\n",
    "        for i in range(self.size):\n",
    "            for j,sim in zip(indices[i],similarities[i]):\n",
    "                if i != j and sim > thres: edges.append((i,j,sim))\n",
    "        self.graph.add_weighted_edges_from(edges)\n",
    "        edge_index = torch.tensor(list(self.graph.edges)).t().contiguous().to(self.device)\n",
    "        self.graph_data = Data(\n",
    "            x=self._get_node_features().clone().detach().to(self.device),\n",
    "            edge_index=edge_index,\n",
    "            y=torch.tensor(self.dataset[\"labels\"].values,dtype=torch.long).to(self.device)\n",
    "        )\n",
    "        train_size = int(1.0 * self.size)\n",
    "        train_mask = torch.zeros(self.size,dtype=torch.bool)\n",
    "        train_mask[torch.randperm(self.size)[:train_size]] = True\n",
    "        self.graph_data.train_mask = train_mask\n",
    "    def update_cluster_labels(self):\n",
    "        self._node_clustering()\n",
    "        self.dataset[\"labels\"] = self.le.fit_transform(self.cluster_labels)\n",
    "        self.gnn_params[\"num_classes\"] = len(self.le.classes_)\n",
    "    def _get_scalar_features(self,idx):\n",
    "        return np.array([\n",
    "            self.dataset[\"is_adult\"][idx] * self.weights[\"is_adult\"],\n",
    "            self.dataset[\"original_language\"][idx] * self.weights[\"original_language\"],\n",
    "            self.dataset[\"year\"][idx] * self.weights[\"year\"]\n",
    "        ])\n",
    "    def _get_node_features(self):\n",
    "        features = []\n",
    "        for i in range(self.size):\n",
    "            scalar = np.expand_dims(self._get_scalar_features(i),axis=0)\n",
    "            text = np.concatenate([\n",
    "                self.dataset[feature][i].reshape(-1,1) * self.weights[feature]\n",
    "                for feature in [\"overview\",\"tagline\",\"genres\"]\n",
    "            ])\n",
    "            combined = np.hstack([scalar,text.reshape(1,-1)])\n",
    "            features.append(combined)\n",
    "        return torch.nn.functional.normalize(\n",
    "            torch.tensor(np.vstack(features),dtype=torch.float).to(self.device),\n",
    "            p=2,dim=1\n",
    "        )\n",
    "    def _init_gnn(self):\n",
    "        self.model = SageRecNet(\n",
    "            input_dim=self.graph_data.x.size(1),\n",
    "            hidden_dim=self.gnn_params[\"hidden_dim\"],\n",
    "            output_dim=self.gnn_params[\"num_classes\"],\n",
    "            num_relations=self.gnn_params[\"num_relations\"]\n",
    "        ).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.gnn_params[\"learning_rate\"],\n",
    "            weight_decay=self.gnn_params[\"weight_decay\"]\n",
    "        )\n",
    "    def _node_clustering(self,clusters=12,use_gnn_embeddings=True):\n",
    "        if use_gnn_embeddings and not hasattr(self.model,\"parameters\"): raise ValueError(\"Train model before GNN-based clustering\")\n",
    "        if use_gnn_embeddings:\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(\n",
    "                    self.graph_data.x,\n",
    "                    self.graph_data.edge_index,\n",
    "                    # self.graph_data.edge_type\n",
    "                ).cpu().numpy()\n",
    "        else: embeddings = self.graph_data.x.cpu().numpy()\n",
    "        kmeans = KMeans(n_clusters=clusters)\n",
    "        self.cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    def _compute_heuristics(self,kn=7,max_weight=0.7,avg_weight=0.3):\n",
    "        self.heuristics = {}\n",
    "        for node in self.graph.nodes():\n",
    "            neighbors = list(self.graph[node].items())\n",
    "            if not neighbors:\n",
    "                self.heuristics[node] = 0\n",
    "                continue\n",
    "            similarities = [data[\"weight\"] for _,data in neighbors]\n",
    "            max_sim = max(similarities)\n",
    "            top_similarities = sorted(similarities,reverse=True)[:kn]\n",
    "            avg_sim = sum(top_similarities) / len(top_similarities)\n",
    "            self.heuristics[node] = (max_sim * max_weight) + (avg_sim * avg_weight)\n",
    "    def train(self,batch_sz=256):\n",
    "        train_loader = NeighborLoader(\n",
    "            self.graph_data,\n",
    "            num_neighbors=[20,10],\n",
    "            batch_size=batch_sz,\n",
    "            shuffle=True,\n",
    "            input_nodes=self.graph_data.train_mask\n",
    "        )\n",
    "        loss_hist = []\n",
    "        for epoch in range(self.gnn_params[\"epochs\"]):\n",
    "            total_loss = 0\n",
    "            self.model.train()\n",
    "            for batch in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.model(batch.x,batch.edge_index)\n",
    "                loss = torch.nn.functional.cross_entropy(out[batch.train_mask],batch.y[batch.train_mask])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{self.gnn_params['epochs']} | Loss: {avg_loss:.4f}\")\n",
    "            if loss_hist and abs(loss_hist[-1] - loss.item()) < 0.001:\n",
    "                print(f\"Converged at epoch {epoch+1}\")\n",
    "                break\n",
    "            loss_hist.append(loss.item())\n",
    "        self.update_cluster_labels()\n",
    "    def _title_match(self,qtitle,thres=4):\n",
    "            query = qtitle.strip().lower()\n",
    "            min_dist = float(\"inf\")\n",
    "            best_idx = -1\n",
    "            for idx,title in enumerate(self.dataset[\"title\"]):\n",
    "                title_clean = str(title).strip().lower()\n",
    "                dist = edit_distance(query,title_clean)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_idx = idx\n",
    "            return best_idx if min_dist <= thres else -1\n",
    "    def gnn_recommendations(self,prompt_titles,k=4):\n",
    "        self.model.eval()\n",
    "        if not hasattr(self,\"cluster_labels\"): self.update_cluster_labels()\n",
    "        input_indices = []\n",
    "        for title in prompt_titles:\n",
    "            idx = self._title_match(title)\n",
    "            if idx == -1:\n",
    "                print(f\"'{title}' not found in dataset\")\n",
    "                continue\n",
    "            input_indices.append(idx)\n",
    "        if not input_indices: return []\n",
    "        with torch.no_grad(): embeddings = self.model(self.graph_data.x,self.graph_data.edge_index)\n",
    "        combined_sims = torch.zeros(len(embeddings)).to(self.device)\n",
    "        for idx in input_indices:\n",
    "            sims = torch.nn.functional.cosine_similarity(embeddings[idx].unsqueeze(0),embeddings)\n",
    "            cluster_mask = self.cluster_labels == self.cluster_labels[idx]\n",
    "            sims[~cluster_mask] *= 0.3\n",
    "            combined_sims += sims\n",
    "        combined_sims /= len(input_indices)\n",
    "        combined_sims[input_indices] = float(\"-inf\")\n",
    "        top_indices = combined_sims.argsort(descending=True)[:k + len(input_indices)]\n",
    "        final_recs = [int(i.item()) for i in top_indices if i not in input_indices][:k]\n",
    "        return [self.dataset[\"title\"].iloc[i] for i in final_recs[:k]]\n",
    "    def a_star_recommendations(self,prompt_titles,k=4,max_depth=4,score_threshold=0.5,max_exploration=500):\n",
    "        if hasattr(self,\"cluster_labels\"): self._node_clustering()  \n",
    "        start_indices = [self._title_match(title.lower()) for title in prompt_titles if title.lower() in self.title_to_index]\n",
    "        if not start_indices: return []\n",
    "        if not hasattr(self,\"heuristics\"): self._compute_heuristics()\n",
    "        node_scores = defaultdict(dict)\n",
    "        exploration_count = 0\n",
    "        queue = []\n",
    "        for start_idx in start_indices:\n",
    "            heapq.heappush(queue,(-self.heuristics[start_idx],start_idx,0.0,0,start_idx))\n",
    "            node_scores[start_idx][start_idx] = 0.0\n",
    "        while queue and exploration_count < max_exploration:\n",
    "            if len(queue) > 1000:\n",
    "                queue = heapq.nsmallest(1000//2,queue)\n",
    "                heapq.heapify(queue)   \n",
    "            current_f_neg,current_node,current_g,depth,origin_idx = heapq.heappop(queue)\n",
    "            current_f = -current_f_neg\n",
    "            if current_f < score_threshold or depth > max_depth:\n",
    "                continue\n",
    "            exploration_count += 1\n",
    "            if current_node not in node_scores or \\\n",
    "            current_g > node_scores[current_node].get(origin_idx,-1):\n",
    "                node_scores[current_node][origin_idx] = current_g\n",
    "            for neighbor in self.graph.neighbors(current_node):\n",
    "                edge_weight = self.graph[current_node][neighbor][\"weight\"]\n",
    "                new_g = current_g + edge_weight\n",
    "                new_f = new_g + self.heuristics[neighbor]\n",
    "\n",
    "                if new_f >= score_threshold:\n",
    "                    if (neighbor not in node_scores) or \\\n",
    "                    (new_g > node_scores[neighbor].get(origin_idx,-1)):\n",
    "                        heapq.heappush(queue,(-new_f,neighbor,new_g,depth+1,origin_idx))\n",
    "        scored_nodes = defaultdict(float)\n",
    "        for node,origins in node_scores.items():\n",
    "            if node in start_indices: continue\n",
    "            avg_score = sum(origins.values()) / len(origins)\n",
    "            scored_nodes[node] = avg_score\n",
    "        sorted_nodes = sorted(scored_nodes.items(),key=lambda x: -x[1])\n",
    "        final_recs = [node for node,score in sorted_nodes[:k + len(start_indices)] if node not in start_indices][:k]\n",
    "        return [self.dataset[\"title\"].iloc[idx] for idx in final_recs[:k]]\n",
    "    def get_recommendations(self,titles,k=4,user_id=\"default\"):\n",
    "        gnn_recs = self.gnn_recommendations(titles,k*2)\n",
    "        a_star_recs = self.a_star_recommendations(titles,k*2)\n",
    "        candidates = list(set(gnn_recs + a_star_recs))\n",
    "        user_context = self._get_user_context(user_id)\n",
    "        final_recs = self._rl_select_action(user_context,candidates,k)\n",
    "        self._update_rl_agent(user_id)\n",
    "        return final_recs\n",
    "    def update_user_feedback(self,user_id,movies,scores,feedback_type=\"explicit\"):\n",
    "        weights = {\n",
    "            \"explicit\": 1.0,\n",
    "            \"implicit\": 0.7,\n",
    "            \"inferred\": 0.5\n",
    "        }\n",
    "        weighted_scores = [s * weights[feedback_type] for s in scores]\n",
    "        feedback = {\n",
    "            \"movies\": movies,\n",
    "            \"scores\": weighted_scores,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"type\": feedback_type\n",
    "        }\n",
    "        self.user_feedback[user_id].append(feedback)\n",
    "        \n",
    "    def _get_user_context(self,user_id):\n",
    "        if user_id not in self.user_feedback: return torch.zeros(self.gnn_params[\"hidden_dim\"]).to(self.device)\n",
    "        all_embs = []\n",
    "        with torch.no_grad():\n",
    "            all_embeddings = self.model(self.graph_data.x,self.graph_data.edge_index)\n",
    "            \n",
    "        for feedback in self.user_feedback[user_id]:\n",
    "            movie_indices = [self.title_to_index[m] for m in feedback[\"movies\"]]\n",
    "            movie_embeddings = all_embeddings[movie_indices]\n",
    "            weighted_embs = movie_embeddings * torch.tensor(\n",
    "                feedback[\"scores\"],device=self.device).unsqueeze(1)\n",
    "            all_embs.append(weighted_embs.mean(dim=0)) \n",
    "        return torch.stack(all_embs).mean(dim=0)\n",
    "    def _rl_select_action(self,user_context,candidates,k=4):\n",
    "        candidate_indices = [self.title_to_index[m] for m in candidates]\n",
    "        with torch.no_grad():\n",
    "            candidate_embeddings = self.model(\n",
    "                self.graph_data.x,\n",
    "                self.graph_data.edge_index\n",
    "            )[candidate_indices]\n",
    "        k = min(k,candidate_embeddings.shape[0])\n",
    "        if np.random.random() < self.epsilon:\n",
    "            selected = np.random.choice(candidate_embeddings.shape[0],k,replace=False)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.rl_agent(user_context,candidate_embeddings).squeeze(1)\n",
    "            selected = q_values.topk(k).indices.cpu().numpy().flatten()\n",
    "        return [candidates[i] for i in selected]\n",
    "    def _update_rl_agent(self,user_id):\n",
    "        if user_id not in self.user_feedback: return\n",
    "        batch_size = 32\n",
    "        feedbacks = self.user_feedback[user_id][-1000:]\n",
    "        if len(feedbacks) < batch_size:\n",
    "            return\n",
    "        batch = random_sample(feedbacks,batch_size)\n",
    "        user_context = self._get_user_context(user_id)\n",
    "        for feedback in batch:\n",
    "            self.rl_agent.optimizer.zero_grad()\n",
    "            movie_indices = [self.title_to_index[m] for m in feedback[\"movies\"]]\n",
    "            movie_embeddings = self.graph_data.x[movie_indices]\n",
    "            predicted_q = self.rl_agent(user_context,movie_embeddings)\n",
    "            \n",
    "            target_q = torch.tensor(feedback[\"scores\"],device=self.device,dtype=torch.float32)\n",
    "            \n",
    "            loss = self.rl_agent.loss_fn(predicted_q,target_q)\n",
    "            loss.backward()\n",
    "            self.rl_agent.optimizer.step()\n",
    "        self.epsilon = max(0.05,self.epsilon * 0.995)\n",
    "    def collect_feedback(self,recommendations,user_id):\n",
    "        print(\"\\nHow relevant are these recommendations? (0.0-1.0)\")\n",
    "        scores = []\n",
    "        for movie in recommendations:\n",
    "            while True:\n",
    "                try:\n",
    "                    score = float(input(f\"{movie}: \"))\n",
    "                    if 0.0 <= score <= 1.0:\n",
    "                        scores.append(score)\n",
    "                        break\n",
    "                    print(\"Please enter between 0.0 (bad) and 1.0 (perfect)\")\n",
    "                except ValueError:\n",
    "                    print(\"Invalid number format\")\n",
    "        self.update_user_feedback(user_id,recommendations,scores)\n",
    "    def save_model(self,flpath,user_data_path=\"../model/user_data.pkl\"):\n",
    "        if not flpath.endswith(\".pkl\"):\n",
    "            flpath += \".pkl\"\n",
    "        with open(flpath,\"wb\") as mdlfile:\n",
    "            pkl_dump({\n",
    "                \"gnn_state\": self.model.state_dict(),\n",
    "                \"rl_state\": self.rl_agent.state_dict(),\n",
    "                \"cluster_labels\": self.cluster_labels,\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"gnn_params\": self.gnn_params\n",
    "            },mdlfile)\n",
    "\n",
    "        with open(user_data_path,\"wb\") as userfile:\n",
    "            pkl_dump({\n",
    "                \"user_feedback\": dict(self.user_feedback),\n",
    "                \"label_encoder\": self.le\n",
    "            },userfile)\n",
    "    @classmethod\n",
    "    def load_model(cls,flpath,dataset,user_data_path=\"../model/user_data.pkl\",device=None):\n",
    "        if not flpath.endswith(\".pkl\"):\n",
    "            flpath += \".pkl\"\n",
    "        with open(flpath,\"rb\") as mdlfile: saved_data = pkl_load(mdlfile)\n",
    "        recommender = cls(dataset,device=device)\n",
    "        recommender.model.load_state_dict(saved_data[\"gnn_state\"])\n",
    "        recommender.rl_agent.load_state_dict(saved_data[\"rl_state\"])\n",
    "        recommender.cluster_labels = saved_data[\"cluster_labels\"]\n",
    "        recommender.epsilon = saved_data[\"epsilon\"]\n",
    "        recommender.gnn_params.update(saved_data[\"gnn_params\"])\n",
    "        try:\n",
    "            with open(user_data_path,\"rb\") as userfile:\n",
    "                user_data = pkl_load(userfile)\n",
    "                recommender.user_feedback = defaultdict(list,user_data[\"user_feedback\"])\n",
    "                recommender.le = user_data[\"label_encoder\"]\n",
    "        except FileNotFoundError: print(\"No user data found,starting fresh\")\n",
    "        return recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47624/3581019618.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.dataset.loc[:,\"labels\"] = kmeans.fit_predict(\n",
      "2025-02-02 15:25:04.275224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738490104.310198   47624 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738490104.320837   47624 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-02 15:25:04.393596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64 | Loss: 2.4505\n",
      "Epoch 2/64 | Loss: 2.3172\n",
      "Epoch 3/64 | Loss: 2.2303\n",
      "Epoch 4/64 | Loss: 2.2262\n",
      "Epoch 5/64 | Loss: 2.2156\n",
      "Epoch 6/64 | Loss: 2.2088\n",
      "Epoch 7/64 | Loss: 2.2059\n",
      "Epoch 8/64 | Loss: 2.2056\n",
      "Epoch 9/64 | Loss: 2.2039\n",
      "Epoch 10/64 | Loss: 2.2044\n",
      "Epoch 11/64 | Loss: 2.2020\n",
      "Epoch 12/64 | Loss: 2.2053\n",
      "Epoch 13/64 | Loss: 2.2014\n",
      "Epoch 14/64 | Loss: 2.2021\n",
      "Epoch 15/64 | Loss: 2.1987\n",
      "Epoch 16/64 | Loss: 2.2015\n",
      "Epoch 17/64 | Loss: 2.2003\n",
      "Converged at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47624/3581019618.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.dataset[\"labels\"] = self.le.fit_transform(self.cluster_labels)\n"
     ]
    }
   ],
   "source": [
    "acc_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "recommender = GraphRecommender(mov_dataset.iloc[:1000],device=acc_device)\n",
    "recommender.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inception', 'the avengers']\n",
      "\n",
      "How relevant are these recommendations? (0.0-1.0)\n",
      "['godzilla: king of the monsters', 'black panther', 'the empire strikes back', 'the matrix resurrections']\n"
     ]
    }
   ],
   "source": [
    "inp_titles = []\n",
    "inp = input(\"Enter a title (leave blank to stop) >>> \")\n",
    "inp_titles.append(inp)\n",
    "while inp.strip() != \"\":\n",
    "    inp = input(\"Enter a title >>> \").strip()\n",
    "    if inp: inp_titles.append(inp)\n",
    "print(inp_titles)\n",
    "recs = recommender.get_recommendations(inp_titles)\n",
    "recommender.collect_feedback(recs,user_id=\"current_user\")\n",
    "pers_recs = recommender.get_recommendations(inp_titles)\n",
    "print(pers_recs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
